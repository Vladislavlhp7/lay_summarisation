{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: packaging in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: responses<0.19 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: aiohttp in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: multiprocess in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: xxhash in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: filelock in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: rouge_score in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from rouge_score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from nltk->rouge_score) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from nltk->rouge_score) (2023.3.23)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /private/var/folders/10/730vyzkx1tn7xdrft3tm6nvw0000gn/T/pip-req-build-hbq970_x\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /private/var/folders/10/730vyzkx1tn7xdrft3tm6nvw0000gn/T/pip-req-build-hbq970_x\n",
      "  Resolved https://github.com/huggingface/transformers to commit 656e869a4523f6a0ce90b3aacbb05cc8fb5794bb\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (0.13.2)\n",
      "Requirement already satisfied: requests in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (3.10.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers==4.28.0.dev0) (2023.3.23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers==4.28.0.dev0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentencepiece in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (0.1.97)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: networkx in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: filelock in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (3.10.7)\n",
      "Requirement already satisfied: sympy in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (4.28.0.dev0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (4.28.0.dev0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: requests in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: multiprocess in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: packaging in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: pandas in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (1.23.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests>=2.19.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: responses<0.19 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: filelock in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (2.12.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (1.53.0)\n",
      "Requirement already satisfied: packaging in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (0.4.8)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (4.22.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ipywidgets in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (8.0.6)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipywidgets) (8.11.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipywidgets) (6.22.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: pyzmq>=20 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: packaging in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: psutil in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: appnope in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: nest-asyncio in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.1.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: decorator in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (6.1.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: pure-eval in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: six in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (3.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mRequirement already satisfied: torch in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: filelock in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from torch) (3.10.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets\n",
    "!pip3 install rouge_score\n",
    "!pip3 install git+https://github.com/huggingface/transformers\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install torch\n",
    "!pip3 install transformers\n",
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade datasets\n",
    "!pip install tensorflow\n",
    "!pip install ipywidgets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/yikuan8/Clinical-Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5  # from paper\n",
    "batch_size = 32\n",
    "max_input_length = 4096\n",
    "max_output_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"yikuan8/Clinical-Longformer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82599e30de114a12a515aa56eb936856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a1f43c419840d381272cb2224a6319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/595M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LongformerForMaskedLM: ['longformer.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LongformerForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"yikuan8/Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"lay_summary\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_article_dataset(dtype, filename, directory):\n",
    "    path = os.path.join(directory, f'{dtype}/{filename}_{dtype}.jsonl')\n",
    "    df = pd.read_json(path, lines=True, nrows=100)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "def create_article_dataset_dict(filename, directory):\n",
    "    dataset_types = ['train', 'val']\n",
    "    datasets = {}\n",
    "    for dtype in dataset_types:\n",
    "        dataset = load_article_dataset(dtype, filename, directory)\n",
    "        # dataset = dataset.map(lambda e: tokenizer(e['lay_summary'], truncation=True, \n",
    "        #                                                   padding='max_length', max_length=max_input_length),batched=True)\n",
    "        # dataset['labels'] = dataset['input_ids']\n",
    "        # dataset = dataset.map(lambda e: tokenizer(e['article'], truncation=True, \n",
    "        #                                                       padding='max_length', max_length=max_input_length), batched=True)\n",
    "        # Apply the preprocess_function to the dataset\n",
    "        dataset = dataset.map(\n",
    "            process_data_to_model_inputs,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            remove_columns=[\"article\", \"lay_summary\", \"headings\"],\n",
    "        )\n",
    "        # Add the preprocessed dataset to the datasets dictionary\n",
    "        dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'global_attention_mask', 'labels'])\n",
    "        datasets[dtype] = dataset\n",
    "    return DatasetDict(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"eLife\"\n",
    "directory = \"../data/task1_development/\"\n",
    "article_dataset = create_article_dataset_dict(filename, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 8\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "\n",
    "def set_seed(seed_v: int = 42) -> None:\n",
    "    np.random.seed(seed_v)\n",
    "    seed(seed_v)\n",
    "    torch.manual_seed(seed_v)\n",
    "    torch.cuda.manual_seed(seed_v)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_v)\n",
    "    print(f\"Random seed set as {seed_v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages (from rouge) (1.16.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mw/.pyenv/versions/3.8.16/envs/nlu/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predictions, labels, avg=True)\n",
    "    return {\n",
    "        \"rouge1_f\": scores[\"rouge-1\"][\"f\"],\n",
    "        \"rouge2_f\": scores[\"rouge-2\"][\"f\"],\n",
    "        \"rougeL_f\": scores[\"rouge-l\"][\"f\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir='../tmp/',\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='rouge2_f',\n",
    "        run_name=model_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=article_dataset['train'],\n",
    "        eval_dataset=article_dataset['val'],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mw/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2699\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2702\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2730\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1848\u001b[0m, in \u001b[0;36mLongformerForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;124;03m['healthy', 'skinny', 'thin', 'good', 'vegetarian']\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1846\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1848\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1860\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1861\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1750\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1742\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)[\n\u001b[1;32m   1743\u001b[0m     :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :\n\u001b[1;32m   1744\u001b[0m ]\n\u001b[1;32m   1746\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1747\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[1;32m   1748\u001b[0m )\n\u001b[0;32m-> 1750\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1759\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1760\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1326\u001b[0m, in \u001b[0;36mLongformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1317\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1318\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1319\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         is_index_global_attn,\n\u001b[1;32m   1324\u001b[0m     )\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1326\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1249\u001b[0m, in \u001b[0;36mLongformerLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1241\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1248\u001b[0m ):\n\u001b[0;32m-> 1249\u001b[0m     self_attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1259\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1185\u001b[0m, in \u001b[0;36mLongformerAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1177\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1184\u001b[0m ):\n\u001b[0;32m-> 1185\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m   1195\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attn_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:574\u001b[0m, in \u001b[0;36mLongformerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    571\u001b[0m query_vectors \u001b[38;5;241m=\u001b[39m query_vectors\u001b[38;5;241m.\u001b[39mview(seq_len, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    572\u001b[0m key_vectors \u001b[38;5;241m=\u001b[39m key_vectors\u001b[38;5;241m.\u001b[39mview(seq_len, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 574\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sliding_chunks_query_key_matmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_sided_attn_window_size\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# values to pad for attention probs\u001b[39;00m\n\u001b[1;32m    579\u001b[0m remove_from_windowed_attention_mask \u001b[38;5;241m=\u001b[39m (attention_mask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/envs/nlu/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:858\u001b[0m, in \u001b[0;36mLongformerSelfAttention._sliding_chunks_query_key_matmul\u001b[0;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[1;32m    849\u001b[0m diagonal_chunked_attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad_and_transpose_last_two_dims(\n\u001b[1;32m    850\u001b[0m     diagonal_chunked_attention_scores, padding\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# allocate space for the overall attention matrix where the chunks are combined. The last dimension\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# window_overlap previous words). The following column is attention score from each word to itself, then\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# followed by window_overlap columns for the upper triangle.\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m diagonal_attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdiagonal_chunked_attention_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m(\n\u001b[1;32m    859\u001b[0m     (batch_size \u001b[38;5;241m*\u001b[39m num_heads, chunks_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, window_overlap, window_overlap \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# - copying the main diagonal and the upper triangle\u001b[39;00m\n\u001b[1;32m    864\u001b[0m diagonal_attention_scores[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, window_overlap:] \u001b[38;5;241m=\u001b[39m diagonal_chunked_attention_scores[\n\u001b[1;32m    865\u001b[0m     :, :, :window_overlap, : window_overlap \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    866\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "617e43f1b5c68eef2bc394177767572eff3551a3ada35aa8bb772cdc611e3810"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
