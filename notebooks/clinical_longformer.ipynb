{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install datasets\n",
    "# !pip3 install rouge_score\n",
    "# !pip3 install git+https://github.com/huggingface/transformers\n",
    "# !pip3 install sentencepiece\n",
    "# !pip3 install torch\n",
    "# !pip3 install transformers\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade datasets\n",
    "# !pip install tensorflow\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/yikuan8/Clinical-Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset, load_metric, DatasetDict, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"yikuan8/Clinical-Longformer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"../Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample, max_input_length: int = 4096, max_target_length: int = 4096):\n",
    "    \"\"\"\n",
    "    Tokenizes the article and summary texts in a given sample and creates a dictionary of model inputs \n",
    "    that can be used for training a language model.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A dictionary containing the article and lay summary texts.\n",
    "        max_input_length (int, optional): The maximum length of the tokenized article text. Defaults to 4096.\n",
    "        max_target_length (int, optional): The maximum length of the tokenized summary text. Defaults to 4096.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the tokenized article text and the tokenized summary text as \"labels\".\n",
    "    \"\"\"\n",
    "    # Tokenize the article text using the provided `max_input_length` and `truncation=True` flag.\n",
    "    model_inputs = tokenizer(sample[\"article\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Tokenize the summary text using the provided `max_target_length` and `truncation=True` flag.\n",
    "    labels = tokenizer(sample[\"lay_summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    # Add the tokenized summary text to the `model_inputs` dictionary with key \"labels\".\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Return the `model_inputs` dictionary as the output of the function.\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clinical(Dataset):\n",
    "    def __init__(self, dtype='train', filename=\"eLife\", dir=\"../data/task1_development/\"):\n",
    "        assert filename in ['PLOS','eLife']\n",
    "        assert dtype in ['train', 'val']\n",
    "        path = os.path.join(dir, f'{dtype}/{filename}_{dtype}.jsonl')\n",
    "        self.df = pd.read_json(path, lines=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        article, lay_summary = self.df.iloc[index][['article', 'lay_summary']]\n",
    "        # if self.transform:\n",
    "        #     sample = {'article': article, 'lay_summary': lay_summary}\n",
    "        #     model_inputs = self.transform(sample)\n",
    "        #     article, lay_summary = model_inputs['article'], model_inputs['lay_summary']\n",
    "        #     return 0\n",
    "        # else:\n",
    "        return article, lay_summary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Clinical(filename=\"eLife\", dtype =\"train\")\n",
    "val_dataset = Clinical(filename=\"eLife\", dtype =\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype='train'\n",
    "filename=\"eLife\" \n",
    "dir=\"../data/task1_development/\"\n",
    "\n",
    "path = os.path.join(dir, f'{dtype}/{filename}_{dtype}.jsonl')\n",
    "df_train = pd.read_json(path, lines=True)\n",
    "\n",
    "dtype='val'\n",
    "path = os.path.join(dir, f'{dtype}/{filename}_{dtype}.jsonl')\n",
    "df_val = pd.read_json(path, lines=True)\n",
    "\n",
    "\n",
    "article_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"val\": Dataset.from_pandas(df_val)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = article_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    article_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = tokenized_datasets[\"val\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 8\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5.6e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
