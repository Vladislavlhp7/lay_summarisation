{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install datasets\n",
    "# !pip3 install rouge_score\n",
    "# !pip3 install git+https://github.com/huggingface/transformers\n",
    "# !pip3 install sentencepiece\n",
    "# !pip3 install torch\n",
    "# !pip3 install transformers\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade datasets\n",
    "# !pip install tensorflow\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/yikuan8/Clinical-Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5  # from paper\n",
    "batch_size = 32\n",
    "max_input_length = 4096\n",
    "max_output_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"yikuan8/Clinical-Longformer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"../Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"lay_summary\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_article_dataset(dtype, filename, directory):\n",
    "    path = os.path.join(directory, f'{dtype}/{filename}_{dtype}.jsonl')\n",
    "    df = pd.read_json(path, lines=True, nrows=100)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "def create_article_dataset_dict(filename, directory):\n",
    "    dataset_types = ['train', 'val']\n",
    "    datasets = {}\n",
    "    for dtype in dataset_types:\n",
    "        dataset = load_article_dataset(dtype, filename, directory)\n",
    "        # dataset = dataset.map(lambda e: tokenizer(e['lay_summary'], truncation=True, \n",
    "        #                                                   padding='max_length', max_length=max_input_length),batched=True)\n",
    "        # dataset['labels'] = dataset['input_ids']\n",
    "        # dataset = dataset.map(lambda e: tokenizer(e['article'], truncation=True, \n",
    "        #                                                       padding='max_length', max_length=max_input_length), batched=True)\n",
    "        # Apply the preprocess_function to the dataset\n",
    "        dataset = dataset.map(\n",
    "            process_data_to_model_inputs,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            remove_columns=[\"article\", \"lay_summary\", \"headings\"],\n",
    "        )\n",
    "        # Add the preprocessed dataset to the datasets dictionary\n",
    "        dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'global_attention_mask', 'labels'])\n",
    "        datasets[dtype] = dataset\n",
    "    return DatasetDict(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"eLife\"\n",
    "directory = \"../data/task1_development/\"\n",
    "article_dataset = create_article_dataset_dict(filename, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 8\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "\n",
    "def set_seed(seed_v: int = 42) -> None:\n",
    "    np.random.seed(seed_v)\n",
    "    seed(seed_v)\n",
    "    torch.manual_seed(seed_v)\n",
    "    torch.cuda.manual_seed(seed_v)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_v)\n",
    "    print(f\"Random seed set as {seed_v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predictions, labels, avg=True)\n",
    "    return {\n",
    "        \"rouge1_f\": scores[\"rouge-1\"][\"f\"],\n",
    "        \"rouge2_f\": scores[\"rouge-2\"][\"f\"],\n",
    "        \"rougeL_f\": scores[\"rouge-l\"][\"f\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir='../tmp/',\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='rouge2_f',\n",
    "        run_name=model_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=article_dataset['train'],\n",
    "        eval_dataset=article_dataset['val'],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "617e43f1b5c68eef2bc394177767572eff3551a3ada35aa8bb772cdc611e3810"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
