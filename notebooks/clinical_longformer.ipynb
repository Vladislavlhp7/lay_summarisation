{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install datasets\n",
    "# !pip3 install rouge_score\n",
    "# !pip3 install git+https://github.com/huggingface/transformers\n",
    "# !pip3 install sentencepiece\n",
    "# !pip3 install torch\n",
    "# !pip3 install transformers\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade datasets\n",
    "# !pip install tensorflow\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/yikuan8/Clinical-Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset, load_metric, DatasetDict, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"yikuan8/Clinical-Longformer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"../Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample, max_input_length: int = 4096, max_target_length: int = 4096):\n",
    "    \"\"\"\n",
    "    Tokenizes the article and summary texts in a given sample and creates a dictionary of model inputs \n",
    "    that can be used for training a language model.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A dictionary containing the article and lay summary texts.\n",
    "        max_input_length (int, optional): The maximum length of the tokenized article text. Defaults to 4096.\n",
    "        max_target_length (int, optional): The maximum length of the tokenized summary text. Defaults to 4096.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the tokenized article text and the tokenized summary text as \"labels\".\n",
    "    \"\"\"\n",
    "    # Tokenize the article text using the provided `max_input_length` and `truncation=True` flag.\n",
    "    model_inputs = tokenizer(sample[\"article\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Tokenize the summary text using the provided `max_target_length` and `truncation=True` flag.\n",
    "    labels = tokenizer(sample[\"lay_summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    # Add the tokenized summary text to the `model_inputs` dictionary with key \"labels\".\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Return the `model_inputs` dictionary as the output of the function.\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_article_dataset(dtype, filename, directory):\n",
    "    path = os.path.join(directory, f'{dtype}/{filename}_{dtype}.jsonl')\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "def create_article_dataset_dict(filename, directory):\n",
    "    dataset_types = ['train', 'val']\n",
    "    datasets = {}\n",
    "    for dtype in dataset_types:\n",
    "        datasets[dtype] = load_article_dataset(dtype, filename, directory)\n",
    "    \n",
    "    return DatasetDict(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"eLife\"\n",
    "directory = \"../data/task1_development/\"\n",
    "article_dataset = create_article_dataset_dict(filename, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = article_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    article_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = tokenized_datasets[\"val\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Code below does not work, due to methods being for tensorflow and model coming from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 8\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5.6e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
