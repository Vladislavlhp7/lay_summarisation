{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjzPvT2TUNAw",
    "outputId": "e5f5bddf-77d6-4965-e151-2c5a94d1ff2c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import LEDConfig\n",
    "from rouge import Rouge\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../laysummarisation\")\n",
    "import laysummarisation\n",
    "from laysummarisation.utils import (\n",
    "    compute_metrics,\n",
    "    create_article_dataset_dict,\n",
    "    set_seed,\n",
    "    load_jsonl_pandas,\n",
    "    load_multiple_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjzPvT2TUNAw",
    "outputId": "e5f5bddf-77d6-4965-e151-2c5a94d1ff2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjzPvT2TUNAw",
    "outputId": "e5f5bddf-77d6-4965-e151-2c5a94d1ff2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lay_summary', 'article', 'headings', 'keywords', 'id'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = glob.glob(os.path.join(\"../data/input/rouge\", \"*.jsonl\"))\n",
    "df = load_multiple_df(all_files)\n",
    "\n",
    "# df.iloc[0].article\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBNGhroiMxzk",
    "outputId": "4095e4d8-9965-42f4-fea7-cbf9c6f85dc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LongformerTokenizer'. \n",
      "The class this function is called from is 'LEDTokenizer'.\n",
      "You are using a model of type longformer to instantiate a model of type led. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LEDForConditionalGeneration: ['longformer.encoder.layer.7.attention.self.query_global.bias', 'longformer.encoder.layer.10.attention.self.value.weight', 'longformer.encoder.layer.2.output.dense.weight', 'longformer.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer.encoder.layer.3.intermediate.dense.weight', 'longformer.encoder.layer.7.attention.self.value_global.bias', 'longformer.encoder.layer.2.attention.self.query_global.bias', 'longformer.encoder.layer.2.attention.self.query.weight', 'longformer.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.query_global.bias', 'longformer.encoder.layer.9.attention.self.key.bias', 'longformer.encoder.layer.5.output.LayerNorm.bias', 'longformer.encoder.layer.7.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.output.dense.bias', 'longformer.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer.encoder.layer.10.attention.self.key.bias', 'longformer.encoder.layer.6.output.dense.bias', 'longformer.encoder.layer.6.attention.self.value.weight', 'longformer.encoder.layer.8.attention.self.value_global.bias', 'longformer.encoder.layer.8.output.dense.weight', 'longformer.encoder.layer.9.attention.self.query.weight', 'longformer.encoder.layer.9.attention.self.query.bias', 'longformer.encoder.layer.4.intermediate.dense.bias', 'longformer.encoder.layer.7.attention.self.value.weight', 'longformer.encoder.layer.6.intermediate.dense.weight', 'longformer.encoder.layer.9.intermediate.dense.weight', 'longformer.encoder.layer.10.attention.self.value_global.weight', 'longformer.encoder.layer.1.attention.self.key.weight', 'longformer.encoder.layer.2.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.self.query_global.bias', 'longformer.encoder.layer.10.attention.output.dense.bias', 'longformer.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.self.key.bias', 'longformer.encoder.layer.0.attention.self.query_global.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.value.bias', 'longformer.encoder.layer.4.attention.self.key.bias', 'longformer.encoder.layer.9.attention.output.dense.bias', 'longformer.encoder.layer.8.attention.self.key.bias', 'longformer.encoder.layer.4.attention.self.key_global.weight', 'longformer.encoder.layer.9.output.LayerNorm.weight', 'longformer.encoder.layer.1.output.dense.bias', 'longformer.encoder.layer.8.attention.self.key.weight', 'longformer.encoder.layer.10.attention.self.query_global.bias', 'longformer.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer.encoder.layer.11.output.dense.weight', 'longformer.encoder.layer.7.intermediate.dense.bias', 'longformer.encoder.layer.8.output.LayerNorm.weight', 'longformer.encoder.layer.0.attention.self.value_global.bias', 'longformer.encoder.layer.0.intermediate.dense.weight', 'longformer.encoder.layer.8.attention.self.query_global.weight', 'longformer.encoder.layer.4.output.LayerNorm.bias', 'longformer.encoder.layer.3.intermediate.dense.bias', 'longformer.encoder.layer.1.attention.self.value_global.bias', 'longformer.encoder.layer.4.attention.self.key_global.bias', 'longformer.embeddings.LayerNorm.bias', 'longformer.encoder.layer.1.attention.self.value.bias', 'longformer.encoder.layer.3.output.dense.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.self.query_global.weight', 'longformer.encoder.layer.7.attention.self.key.bias', 'longformer.encoder.layer.7.attention.self.key.weight', 'longformer.encoder.layer.2.attention.self.key_global.bias', 'longformer.encoder.layer.5.attention.output.dense.bias', 'longformer.encoder.layer.2.attention.self.value_global.bias', 'longformer.encoder.layer.4.attention.self.value.bias', 'longformer.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer.encoder.layer.11.attention.self.query_global.bias', 'longformer.encoder.layer.11.intermediate.dense.weight', 'longformer.embeddings.word_embeddings.weight', 'longformer.encoder.layer.11.attention.output.dense.bias', 'longformer.encoder.layer.5.output.dense.weight', 'longformer.embeddings.position_embeddings.weight', 'longformer.encoder.layer.3.attention.self.key.bias', 'longformer.encoder.layer.4.attention.self.value_global.bias', 'longformer.encoder.layer.7.attention.self.query.bias', 'longformer.embeddings.position_ids', 'longformer.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.self.value.bias', 'longformer.encoder.layer.4.attention.self.query.bias', 'longformer.encoder.layer.5.attention.self.value_global.bias', 'longformer.encoder.layer.11.output.LayerNorm.weight', 'longformer.encoder.layer.0.attention.self.key_global.weight', 'longformer.encoder.layer.1.output.dense.weight', 'longformer.encoder.layer.10.attention.self.key_global.weight', 'longformer.encoder.layer.11.intermediate.dense.bias', 'lm_head.bias', 'longformer.encoder.layer.0.attention.self.query.weight', 'longformer.encoder.layer.6.attention.self.key.bias', 'longformer.encoder.layer.5.attention.self.key.bias', 'longformer.encoder.layer.5.output.dense.bias', 'longformer.encoder.layer.7.attention.self.key_global.weight', 'longformer.encoder.layer.7.output.LayerNorm.weight', 'longformer.encoder.layer.5.attention.self.key.weight', 'longformer.encoder.layer.9.attention.self.query_global.weight', 'longformer.encoder.layer.11.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.self.key_global.weight', 'longformer.encoder.layer.2.attention.output.dense.bias', 'longformer.encoder.layer.1.attention.self.key_global.bias', 'longformer.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer.encoder.layer.6.intermediate.dense.bias', 'longformer.encoder.layer.8.output.dense.bias', 'longformer.encoder.layer.3.attention.self.value_global.weight', 'longformer.encoder.layer.0.attention.self.key.weight', 'longformer.encoder.layer.9.intermediate.dense.bias', 'longformer.encoder.layer.11.attention.self.value.bias', 'longformer.encoder.layer.0.intermediate.dense.bias', 'longformer.embeddings.LayerNorm.weight', 'longformer.encoder.layer.1.attention.output.dense.bias', 'longformer.encoder.layer.1.output.LayerNorm.bias', 'longformer.encoder.layer.1.attention.output.dense.weight', 'longformer.encoder.layer.1.attention.self.value.weight', 'longformer.encoder.layer.7.attention.self.query.weight', 'longformer.encoder.layer.8.attention.self.query.bias', 'longformer.encoder.layer.10.output.dense.weight', 'longformer.encoder.layer.3.attention.self.key_global.bias', 'longformer.encoder.layer.2.output.dense.bias', 'longformer.encoder.layer.3.attention.self.query.bias', 'longformer.encoder.layer.4.output.LayerNorm.weight', 'longformer.encoder.layer.5.output.LayerNorm.weight', 'longformer.encoder.layer.2.intermediate.dense.weight', 'longformer.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer.encoder.layer.0.output.LayerNorm.bias', 'longformer.encoder.layer.6.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.self.key_global.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.output.dense.weight', 'lm_head.dense.bias', 'longformer.encoder.layer.3.output.LayerNorm.weight', 'longformer.encoder.layer.8.intermediate.dense.bias', 'longformer.encoder.layer.9.attention.self.value.weight', 'longformer.encoder.layer.2.attention.self.value.weight', 'longformer.encoder.layer.10.intermediate.dense.weight', 'longformer.encoder.layer.6.attention.self.value_global.weight', 'longformer.encoder.layer.8.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.key.bias', 'longformer.encoder.layer.11.attention.self.query.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.attention.self.key_global.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer.encoder.layer.4.output.dense.bias', 'longformer.encoder.layer.1.intermediate.dense.bias', 'longformer.embeddings.token_type_embeddings.weight', 'longformer.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer.encoder.layer.11.attention.self.key.weight', 'longformer.encoder.layer.2.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.self.query.weight', 'longformer.encoder.layer.4.attention.output.dense.weight', 'longformer.encoder.layer.10.attention.self.value_global.bias', 'longformer.encoder.layer.7.attention.self.value_global.weight', 'longformer.encoder.layer.10.attention.self.value.bias', 'lm_head.layer_norm.bias', 'longformer.encoder.layer.9.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.self.key_global.weight', 'longformer.encoder.layer.0.output.dense.weight', 'longformer.encoder.layer.1.attention.self.value_global.weight', 'longformer.encoder.layer.5.attention.self.query_global.weight', 'longformer.encoder.layer.7.output.dense.weight', 'longformer.encoder.layer.6.attention.self.value_global.bias', 'longformer.encoder.layer.9.attention.self.key_global.bias', 'longformer.encoder.layer.4.attention.self.query.weight', 'longformer.encoder.layer.5.attention.output.dense.weight', 'longformer.encoder.layer.1.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.key.weight', 'longformer.encoder.layer.4.attention.self.value.weight', 'longformer.encoder.layer.1.attention.self.query.bias', 'longformer.encoder.layer.5.attention.self.query.bias', 'longformer.encoder.layer.2.attention.self.query_global.weight', 'longformer.encoder.layer.8.attention.output.dense.bias', 'longformer.encoder.layer.0.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.self.query_global.weight', 'longformer.encoder.layer.0.attention.self.value_global.weight', 'longformer.encoder.layer.3.attention.self.key.weight', 'longformer.encoder.layer.6.attention.self.key.weight', 'longformer.encoder.layer.9.attention.self.key.weight', 'longformer.encoder.layer.5.attention.self.value.weight', 'longformer.encoder.layer.6.attention.self.query.bias', 'longformer.encoder.layer.9.output.dense.bias', 'longformer.encoder.layer.5.intermediate.dense.bias', 'longformer.encoder.layer.2.attention.self.key.weight', 'longformer.encoder.layer.8.output.LayerNorm.bias', 'longformer.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.value_global.weight', 'lm_head.decoder.bias', 'longformer.encoder.layer.3.attention.self.query.weight', 'longformer.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer.encoder.layer.4.output.dense.weight', 'longformer.encoder.layer.11.attention.self.query.weight', 'longformer.encoder.layer.0.output.dense.bias', 'longformer.encoder.layer.7.attention.output.dense.weight', 'longformer.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer.encoder.layer.4.attention.self.key.weight', 'longformer.encoder.layer.10.attention.self.key_global.bias', 'longformer.encoder.layer.11.attention.self.value_global.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer.encoder.layer.3.attention.self.value.weight', 'longformer.encoder.layer.6.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.value_global.weight', 'longformer.encoder.layer.5.attention.self.key_global.weight', 'longformer.encoder.layer.5.attention.self.query.weight', 'longformer.encoder.layer.5.intermediate.dense.weight', 'longformer.encoder.layer.1.attention.self.query_global.weight', 'longformer.encoder.layer.6.attention.output.dense.bias', 'longformer.encoder.layer.3.attention.output.dense.bias', 'longformer.encoder.layer.11.attention.self.key_global.bias', 'longformer.encoder.layer.0.attention.self.query.bias', 'longformer.encoder.layer.2.attention.self.value_global.weight', 'longformer.encoder.layer.10.intermediate.dense.bias', 'longformer.encoder.layer.11.output.dense.bias', 'longformer.encoder.layer.2.attention.self.key_global.weight', 'longformer.encoder.layer.4.attention.self.query_global.bias', 'longformer.encoder.layer.9.attention.self.key_global.weight', 'longformer.encoder.layer.8.attention.self.value_global.weight', 'longformer.encoder.layer.7.attention.self.value.bias', 'longformer.encoder.layer.7.intermediate.dense.weight', 'longformer.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer.encoder.layer.8.attention.self.key_global.bias', 'longformer.encoder.layer.11.attention.self.key_global.weight', 'longformer.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.self.value_global.bias', 'longformer.encoder.layer.7.attention.output.dense.bias', 'longformer.encoder.layer.9.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.value.weight', 'longformer.encoder.layer.8.attention.self.query_global.bias', 'longformer.encoder.layer.11.attention.self.query_global.weight', 'longformer.encoder.layer.8.attention.self.query.weight', 'longformer.encoder.layer.11.attention.self.value_global.bias', 'longformer.encoder.layer.10.output.dense.bias', 'longformer.encoder.layer.3.output.LayerNorm.bias', 'longformer.encoder.layer.4.intermediate.dense.weight', 'longformer.encoder.layer.6.output.LayerNorm.weight', 'longformer.encoder.layer.6.output.dense.weight', 'longformer.encoder.layer.9.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.self.query.bias', 'longformer.encoder.layer.6.attention.self.query.weight', 'longformer.encoder.layer.2.attention.output.dense.weight', 'longformer.encoder.layer.4.attention.output.dense.bias', 'longformer.encoder.layer.10.attention.self.query.weight', 'longformer.encoder.layer.4.attention.self.value_global.weight', 'lm_head.dense.weight', 'longformer.encoder.layer.2.attention.self.query.bias', 'longformer.encoder.layer.8.attention.self.value.bias', 'longformer.encoder.layer.9.attention.output.dense.weight', 'longformer.encoder.layer.10.output.LayerNorm.bias', 'longformer.encoder.layer.2.attention.self.value.bias', 'longformer.encoder.layer.7.output.dense.bias', 'longformer.encoder.layer.1.intermediate.dense.weight', 'longformer.encoder.layer.2.attention.self.key.bias', 'longformer.encoder.layer.2.intermediate.dense.bias', 'longformer.encoder.layer.3.output.dense.weight', 'longformer.encoder.layer.10.attention.self.query_global.weight', 'longformer.encoder.layer.0.attention.self.value.bias', 'longformer.encoder.layer.5.attention.self.key_global.bias', 'longformer.encoder.layer.9.output.dense.weight', 'longformer.encoder.layer.0.attention.self.value.weight', 'longformer.encoder.layer.3.attention.output.dense.weight', 'longformer.encoder.layer.1.attention.self.query_global.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.self.key_global.bias', 'longformer.encoder.layer.11.attention.output.dense.weight', 'lm_head.decoder.weight', 'longformer.encoder.layer.1.attention.self.key.bias', 'longformer.encoder.layer.6.attention.self.value.bias', 'longformer.encoder.layer.9.attention.self.query_global.bias', 'longformer.encoder.layer.10.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'longformer.encoder.layer.4.attention.self.query_global.weight', 'longformer.encoder.layer.6.attention.self.key_global.weight', 'longformer.encoder.layer.0.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.self.key_global.weight', 'longformer.encoder.layer.7.attention.self.query_global.weight', 'longformer.encoder.layer.6.attention.self.query_global.weight', 'longformer.encoder.layer.11.attention.self.value.weight', 'longformer.encoder.layer.6.attention.self.query_global.bias', 'longformer.encoder.layer.10.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing LEDForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEDForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and evaluation sets\n",
    "train_df, eval_df = train_test_split(df.head(100), test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "model_checkpoint = \"yikuan8/Clinical-Longformer\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Create the Longformer configuration\n",
    "lf_config = LEDConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "# Update the attention_window parameter\n",
    "lf_config.attention_window = [16] * lf_config.num_hidden_layers\n",
    "model.config.num_beams = conf.nbeams\n",
    "model.config.max_length = conf.max_encode\n",
    "model.config.min_length = conf.min_encode\n",
    "model.config.length_penalty = conf.length_penalty\n",
    "model.config.early_stopping = conf.early_stopping\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_checkpoint, config=lf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SZWNGFYnNgnM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class eLifeDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_input_length=1024, max_output_length=64):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        article, lay_summary = row['article'], row['lay_summary']\n",
    "\n",
    "        input_tokenized = self.tokenizer(\n",
    "            article,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        target_tokenized = self.tokenizer(\n",
    "            lay_summary,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_output_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        input_ids = input_tokenized[\"input_ids\"].squeeze()\n",
    "        target_ids = target_tokenized[\"input_ids\"].squeeze()\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"labels\": target_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "Qrsu9hWbM7t-",
    "outputId": "ec5fc2cd-76cc-4574-ed8b-67387d439123"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 03:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.846500</td>\n",
       "      <td>8.059049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.419600</td>\n",
       "      <td>7.668071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6.769100</td>\n",
       "      <td>7.264815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.044500</td>\n",
       "      <td>7.025229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('longformer_summary_model/tokenizer_config.json',\n",
       " 'longformer_summary_model/special_tokens_map.json',\n",
       " 'longformer_summary_model/vocab.json',\n",
       " 'longformer_summary_model/merges.txt',\n",
       " 'longformer_summary_model/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    output_dir=\"longformer_summary_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = eLifeDataset(train_df, tokenizer)\n",
    "eval_dataset = eLifeDataset(eval_df, tokenizer)\n",
    "\n",
    "# Set up the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    "  )\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"longformer_summary_model\")\n",
    "tokenizer.save_pretrained(\"longformer_summary_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWeI9WWJZL0P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
